{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b9de3cb-3556-4c97-b887-aa68d2f29e27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Guide on Creating Metadata for V3 and Performing Data Query and Extraction\n",
    "\n",
    "Author: [Wong Songhan](mailto:songhan89@gmail.com)\n",
    "\n",
    "Date: 16-06-2024\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This guide demonstrates how to use `xarray`, `zarr`, and `Virtualizarr` to create a virtual dataset of V3 data stored in a virtual Zarr format. This approach facilitates efficient data querying and extraction. For computation, we use `dask` to parallelize the data extraction process.\n",
    "\n",
    "For insights into the rationale behind these technology choices, refer to the following link:\n",
    "* [Problem with Data Access for Large-Scale Weather and Climate Data](https://tomaugspurger.net/noaa-nwm/02-problems.html)\n",
    "\n",
    "The guide shown here is using a local file system and local Dask cluster. The same idea would also work with cloud storage and cloud Dask cluster.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10 or higher\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install the required packages, run the following command:\n",
    "\n",
    "```bash\n",
    "pip install xarray[complete] kerchunk dask dask-gateway\n",
    "```\n",
    "\n",
    "To install the Virtualizarr package, use these commands:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/TomNicholas/VirtualiZarr\n",
    "cd VirtualiZarr\n",
    "pip install .\n",
    "```\n",
    "\n",
    "## Virtualizarr\n",
    "\n",
    "In this guide, we use [Virtualizarr](https://virtualizarr.readthedocs.io/en/latest/) instead of Kerchunk to create a virtual dataset. Virtualizarr simplifies the technical complexity of Kerchunk, offering a more user-friendly interface.\n",
    "\n",
    "For those interested in using Kerchunk, here are some useful resources:\n",
    "* [Using Kerchunk](https://tomaugspurger.net/noaa-nwm/03-using-kerchunk.html)\n",
    "* [Kerchunk Cookbook](https://github.com/ProjectPythia/kerchunk-cookbook)\n",
    "\n",
    "## Dask Cluster\n",
    "\n",
    "This guide uses a local Dask cluster. For scalable TB-scale workloads, consider setting up a dedicated Dask cluster using Azure Kubernetes Service (AKS).\n",
    "\n",
    "Microsoft Planetary Computer provides an excellent guide on setting up a Dask cluster on Azure, which you can find here:\n",
    "* [Setting up Dask Cluster on Azure](https://planetarycomputer.microsoft.com/docs/concepts/hub-deployment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ed63d4-6c4f-44d9-91f9-111ed103d1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from glob import glob\n",
    "from virtualizarr import open_virtual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2c29bea-8224-4b3c-9cbb-822f731a8d06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 1: Creating Metadata for V3 Data\n",
    "\n",
    "First, we need to create metadata for the V3 data. This is a one-time process that involves creating a virtual dataset that points to the netcdf data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16938e44-00eb-4400-86e1-97c806577ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check size of the dataset\n",
    "!du -sh ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "600f7d4d-a6dc-4c1e-9865-c17b7edfca07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find all *.nc files in the data folder recursively\n",
    "files = glob('data/**/*.nc', recursive=True)\n",
    "files = sorted(files)\n",
    "files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40774fa0-eab1-44a1-b895-c7c04fd19710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we will be reading the metadata from the V3 netcdf data. This is a lazy operation and does not load the full datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c856d36b-e714-4bc2-8414-eb4ef57905a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# open all files as virtual datasets\n",
    "# this is a lazy operation, no data is read yet\n",
    "list_of_virtual_datasets = [open_virtual_dataset(f) for f in files]\n",
    "list_of_virtual_datasets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "716652ae-4684-4b12-8f0e-5711e90e3975",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preview one of the datasets as a xarray dataset\n",
    "list_of_virtual_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46c4154-e6b3-41f1-be13-44ee7978d10f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 2: Combine the virtual datasets into a single dataset\n",
    "\n",
    "Now that we have loaded the individual netCDF files into a virtual datasets, we can combine them into a single dataset. This is done by saving a json metadata that describes the full set of virtual datasets. This json metadata can be used to load the full virtual dataset in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6890615-8d87-4243-b287-b57674574e9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_ds = list_of_virtual_datasets[0]\n",
    "\n",
    "for ds in list_of_virtual_datasets[1:]:\n",
    "    temp_ds = xr.concat([temp_ds, ds], dim='time')\n",
    "\n",
    "virtual_ds = temp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a134ddd6-9715-4438-8963-19326be8cbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "virtual_ds.virtualize.to_kerchunk('v3_combined.json', format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "751d8469-0275-4075-ba12-7e2a2f3465da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 3: Load the combined virtual datasets \n",
    "\n",
    "Now, let's pretend that we are a user who wish to do perform slice and dice operations on the combined dataset. \n",
    "We can load the combined virtual dataset using the xarray `open_dataset` function and perform operations on it.\n",
    "\n",
    "You can see that we can load the entire dataset in a lazy manner quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b690b45-f7b9-4485-8a66-bad355f28289",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Set up a local Dask cluster with a memory limit for each worker (e.g., 2 GB per worker)\n",
    "cluster = LocalCluster(n_workers=4, memory_limit='2GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b837fb-6a89-475c-89a5-20bce8124a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#client.start()\n",
    "#client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487753fb-b7f7-4894-8c98-6dd0c1275574",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# click on URL below to view the Dask dashboard\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c325c3-c634-47d2-929c-0c47b4047c16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the virtual dataset as a xarray dataset and sort by time\n",
    "import xarray as xr\n",
    "# you may need to experiment with the chunk size to find the optimal value, depending on the configuration of your cluster\n",
    "v3_ds = xr.open_dataset('v3_combined.json', engine=\"kerchunk\", chunks={\n",
    "    'time': 20,\n",
    "    'lat': 20,\n",
    "    'lon': 20\n",
    "}).sortby('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76600f77-af88-4559-b81d-648f6215fee6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check the size of the combined dataset\n",
    "print (f\"Size of data: {v3_ds.nbytes / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93503a95-7dfd-43d0-8fd5-9d7173116c5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 4: Perform data query and extraction\n",
    "\n",
    "In this case, we will extract the data for a specific time range and spatial extent. This is done by using the `sel` method in xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccbfd918-d731-43c5-a612-99519d698bd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select a subset of the data\n",
    "lon_min, lon_max = 90, 100\n",
    "lat_min, lat_max = -10, 10\n",
    "time_min, time_max = '2015-01-01', '2015-12-31'\n",
    "v3_ds_subset = v3_ds.sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max), time=slice(time_min, time_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65bf28c7-f44e-482c-babd-6c5ce5d39728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now preview the metadata of the extracted data. No operation has been done on the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf3ef4c-3ec2-4224-8eb5-6c202d279581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v3_ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57fc7105-b883-40c4-8538-a720677ebdad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 5: Perform data extraction and export to netcdf\n",
    "\n",
    "Finally, we can extract the data and export it to a netcdf file. This is done by using the `to_netcdf` method in xarray.\n",
    "\n",
    "When you run this code, you will see that the data extraction process is parallelized using Dask. This is because we are using a Dask cluster to perform the data extraction.\n",
    "\n",
    "<img src='./images/dask-compute.png' width='50% px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e57367-5dbb-46a5-abb3-0e766d28f0a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if file does not exist, save the subset to a netcdf file\n",
    "# else delete it \n",
    "import os\n",
    "\n",
    "try:\n",
    "    v3_ds_subset.to_netcdf('./output/v3_subset.nc')\n",
    "except:\n",
    "    os.remove('./output/v3_subset.nc')\n",
    "    v3_ds_subset.to_netcdf('./output/v3_subset.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87a9005-d111-4623-ab8d-987eec8ac010",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Step: Inspect the extracted netCDF data\n",
    "\n",
    "Now that our data is saved to a netCDF file, we can inspect it using xarray and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be50f1e8-c348-4afc-8fdf-d699ed0687ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "# open the extracted netcdf file\n",
    "\n",
    "v3_ds_subset = xr.open_dataset('./output/v3_subset.nc')\n",
    "v3_ds_subset.sel(time='2015-01-01').pr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe4df11-2962-41fa-bcc9-10d3ce489bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# shutdown the Dask cluster\n",
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "v3_data_catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
